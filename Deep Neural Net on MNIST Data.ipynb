{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Neural Net on MNIST Data\n",
    "\n",
    "This notebook shows an implementation of the deep neural network discussed on the bitfusion blog post titled \"Intro to Tensorflow.\" More details about the code can be found there.\n",
    "\n",
    "What follows assumes you are set up on the bitfusion TensorFlow 1.0 AMI and that you have already run the `./setup.sh` script to download the required data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set up the Environment\n",
    "\n",
    "The first thing we do is import the required packages and also change the logging of TensorFlow (TF). Since tf.contrib.learn is still part of contrib and not yet core, there are some warning messages that are outputted. If the warning in these messages becomes something that needs to change the code, we will change the code. The commented out code is what would be in place if the TF code was not causing a large amount of warning text (and what should be used in the future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in required packages\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import tensorflow.contrib.learn as learn\n",
    "from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec\n",
    "import tensorflow.contrib.metrics as tfmetrics\n",
    "import cPickle\n",
    "import gzip\n",
    "\n",
    "# The following code is to over-write the logging information outputted by tf.contrib.learn\n",
    "from logging import StreamHandler, INFO, getLogger\n",
    "\n",
    "logger = getLogger('tensorflow')\n",
    "logger.removeHandler(logger.handlers[0])\n",
    "\n",
    "logger.setLevel(INFO)\n",
    "\n",
    "\n",
    "class DebugFileHandler(StreamHandler):\n",
    "    def __init__(self):\n",
    "        StreamHandler.__init__(self)\n",
    "\n",
    "    def emit(self, record):\n",
    "        if not record.levelno == INFO:\n",
    "            return\n",
    "        StreamHandler.emit(self, record)\n",
    "\n",
    "logger.addHandler(DebugFileHandler())\n",
    "\n",
    "# Once the code is fixed, the way that the code should be implemented is this:\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Reading the Data\n",
    "\n",
    "For this project we will be reading in the MNIST dataset that was downloaded as part of the `./setup.sh` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read Data\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Defining the Model Using `tf.contrib.learn`\n",
    "\n",
    "We will now define the model structure for a deep neural network (DNN). The `tf.contrib.learn` framework provides pre-packaged models that can be run in a very similar manner to a scikit-learn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default config.\n",
      "Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f134c1dd750>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "# Infer the feature columns\n",
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(train_set[0])\n",
    "\n",
    "# Define the DNN classifier\n",
    "classifier = tf.contrib.learn.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[512, 128],\n",
    "    n_classes=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training the `tf.contrib.learn` Model\n",
    "\n",
    "The next thing that we will do is train the model using the `fit` function on the `DNNClassifer` class object. The code will run for 10,000 steps with a batch size of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n",
      "Create CheckpointSaverHook.\n",
      "Saving checkpoints for 1 into /tmp/tmpl6L7el/model.ckpt.\n",
      "loss = 2.38357, step = 1\n",
      "global_step/sec: 291.04\n",
      "loss = 0.364843, step = 101\n",
      "global_step/sec: 296.332\n",
      "loss = 0.293239, step = 201\n",
      "global_step/sec: 294.974\n",
      "loss = 0.162038, step = 301\n",
      "global_step/sec: 294.066\n",
      "loss = 0.306019, step = 401\n",
      "global_step/sec: 290.946\n",
      "loss = 0.146603, step = 501\n",
      "global_step/sec: 300.217\n",
      "loss = 0.276031, step = 601\n",
      "global_step/sec: 294.977\n",
      "loss = 0.158162, step = 701\n",
      "global_step/sec: 297.202\n",
      "loss = 0.178328, step = 801\n",
      "global_step/sec: 300.972\n",
      "loss = 0.092749, step = 901\n",
      "global_step/sec: 295.86\n",
      "loss = 0.163923, step = 1001\n",
      "global_step/sec: 297.505\n",
      "loss = 0.0521841, step = 1101\n",
      "global_step/sec: 300.212\n",
      "loss = 0.0600116, step = 1201\n",
      "global_step/sec: 296.639\n",
      "loss = 0.0985562, step = 1301\n",
      "global_step/sec: 299.17\n",
      "loss = 0.0707446, step = 1401\n",
      "global_step/sec: 297.624\n",
      "loss = 0.0718092, step = 1501\n",
      "global_step/sec: 299.183\n",
      "loss = 0.0541069, step = 1601\n",
      "global_step/sec: 296.721\n",
      "loss = 0.0786767, step = 1701\n",
      "global_step/sec: 298.548\n",
      "loss = 0.0520596, step = 1801\n",
      "global_step/sec: 297.968\n",
      "loss = 0.0958099, step = 1901\n",
      "global_step/sec: 297.188\n",
      "loss = 0.0807805, step = 2001\n",
      "global_step/sec: 300.163\n",
      "loss = 0.0443349, step = 2101\n",
      "global_step/sec: 299.024\n",
      "loss = 0.0867677, step = 2201\n",
      "global_step/sec: 302.795\n",
      "loss = 0.189983, step = 2301\n",
      "global_step/sec: 299.312\n",
      "loss = 0.0926261, step = 2401\n",
      "global_step/sec: 295.728\n",
      "loss = 0.0479553, step = 2501\n",
      "global_step/sec: 297.86\n",
      "loss = 0.030344, step = 2601\n",
      "global_step/sec: 295.898\n",
      "loss = 0.0517422, step = 2701\n",
      "global_step/sec: 297.181\n",
      "loss = 0.0329515, step = 2801\n",
      "global_step/sec: 299.653\n",
      "loss = 0.0525349, step = 2901\n",
      "global_step/sec: 298.881\n",
      "loss = 0.0202657, step = 3001\n",
      "global_step/sec: 291.931\n",
      "loss = 0.0255708, step = 3101\n",
      "global_step/sec: 299.525\n",
      "loss = 0.0703368, step = 3201\n",
      "global_step/sec: 297.622\n",
      "loss = 0.0480483, step = 3301\n",
      "global_step/sec: 297.538\n",
      "loss = 0.017764, step = 3401\n",
      "global_step/sec: 295.47\n",
      "loss = 0.0172406, step = 3501\n",
      "global_step/sec: 299.967\n",
      "loss = 0.029015, step = 3601\n",
      "global_step/sec: 301.962\n",
      "loss = 0.0668356, step = 3701\n",
      "global_step/sec: 295.08\n",
      "loss = 0.0201284, step = 3801\n",
      "global_step/sec: 301.023\n",
      "loss = 0.0350203, step = 3901\n",
      "global_step/sec: 293.802\n",
      "loss = 0.029518, step = 4001\n",
      "global_step/sec: 293.59\n",
      "loss = 0.0132639, step = 4101\n",
      "global_step/sec: 299.142\n",
      "loss = 0.0170319, step = 4201\n",
      "global_step/sec: 297.552\n",
      "loss = 0.0163715, step = 4301\n",
      "global_step/sec: 299.5\n",
      "loss = 0.110028, step = 4401\n",
      "global_step/sec: 297.101\n",
      "loss = 0.0469119, step = 4501\n",
      "global_step/sec: 302.091\n",
      "loss = 0.00697435, step = 4601\n",
      "global_step/sec: 297\n",
      "loss = 0.00529607, step = 4701\n",
      "global_step/sec: 300.732\n",
      "loss = 0.0216766, step = 4801\n",
      "global_step/sec: 298.922\n",
      "loss = 0.012739, step = 4901\n",
      "global_step/sec: 297.585\n",
      "loss = 0.0138437, step = 5001\n",
      "global_step/sec: 297.794\n",
      "loss = 0.00819549, step = 5101\n",
      "global_step/sec: 298.153\n",
      "loss = 0.012001, step = 5201\n",
      "global_step/sec: 297.752\n",
      "loss = 0.0126909, step = 5301\n",
      "global_step/sec: 298.861\n",
      "loss = 0.0147001, step = 5401\n",
      "global_step/sec: 293.008\n",
      "loss = 0.0134156, step = 5501\n",
      "global_step/sec: 292.754\n",
      "loss = 0.0130935, step = 5601\n",
      "global_step/sec: 300.152\n",
      "loss = 0.0101712, step = 5701\n",
      "global_step/sec: 299.195\n",
      "loss = 0.0198511, step = 5801\n",
      "global_step/sec: 295.73\n",
      "loss = 0.00734607, step = 5901\n",
      "global_step/sec: 296.149\n",
      "loss = 0.0270081, step = 6001\n",
      "global_step/sec: 292.241\n",
      "loss = 0.00351676, step = 6101\n",
      "global_step/sec: 289.838\n",
      "loss = 0.0100108, step = 6201\n",
      "global_step/sec: 295.135\n",
      "loss = 0.00415879, step = 6301\n",
      "global_step/sec: 296.002\n",
      "loss = 0.00253935, step = 6401\n",
      "global_step/sec: 294.924\n",
      "loss = 0.0046002, step = 6501\n",
      "global_step/sec: 294.776\n",
      "loss = 0.00486408, step = 6601\n",
      "global_step/sec: 294.6\n",
      "loss = 0.00611634, step = 6701\n",
      "global_step/sec: 292.774\n",
      "loss = 0.0127234, step = 6801\n",
      "global_step/sec: 297.992\n",
      "loss = 0.0090427, step = 6901\n",
      "global_step/sec: 291.743\n",
      "loss = 0.00409297, step = 7001\n",
      "global_step/sec: 291.752\n",
      "loss = 0.00519878, step = 7101\n",
      "global_step/sec: 290.698\n",
      "loss = 0.0038585, step = 7201\n",
      "global_step/sec: 294.622\n",
      "loss = 0.00376067, step = 7301\n",
      "global_step/sec: 295.319\n",
      "loss = 0.00659992, step = 7401\n",
      "global_step/sec: 293.859\n",
      "loss = 0.0100231, step = 7501\n",
      "global_step/sec: 295.208\n",
      "loss = 0.00440595, step = 7601\n",
      "global_step/sec: 298.967\n",
      "loss = 0.0160552, step = 7701\n",
      "global_step/sec: 299.545\n",
      "loss = 0.00243347, step = 7801\n",
      "global_step/sec: 296.452\n",
      "loss = 0.00184946, step = 7901\n",
      "global_step/sec: 295.397\n",
      "loss = 0.00275352, step = 8001\n",
      "global_step/sec: 296.337\n",
      "loss = 0.00250187, step = 8101\n",
      "global_step/sec: 302.505\n",
      "loss = 0.00359899, step = 8201\n",
      "global_step/sec: 293.984\n",
      "loss = 0.00666373, step = 8301\n",
      "global_step/sec: 298.327\n",
      "loss = 0.00312092, step = 8401\n",
      "global_step/sec: 295.984\n",
      "loss = 0.00186003, step = 8501\n",
      "global_step/sec: 290.57\n",
      "loss = 0.00193991, step = 8601\n",
      "global_step/sec: 302.653\n",
      "loss = 0.00285032, step = 8701\n",
      "global_step/sec: 296.941\n",
      "loss = 0.00237241, step = 8801\n",
      "global_step/sec: 293.994\n",
      "loss = 0.00545197, step = 8901\n",
      "global_step/sec: 301.077\n",
      "loss = 0.0109321, step = 9001\n",
      "global_step/sec: 298.555\n",
      "loss = 0.00440491, step = 9101\n",
      "global_step/sec: 298.563\n",
      "loss = 0.00385791, step = 9201\n",
      "global_step/sec: 300.553\n",
      "loss = 0.00347972, step = 9301\n",
      "global_step/sec: 297.281\n",
      "loss = 0.00203248, step = 9401\n",
      "global_step/sec: 297.941\n",
      "loss = 0.00366369, step = 9501\n",
      "global_step/sec: 294.047\n",
      "loss = 0.0016268, step = 9601\n",
      "global_step/sec: 296.783\n",
      "loss = 0.00454907, step = 9701\n",
      "global_step/sec: 296.877\n",
      "loss = 0.00201933, step = 9801\n",
      "global_step/sec: 294.428\n",
      "loss = 0.00172086, step = 9901\n",
      "Saving checkpoints for 10000 into /tmp/tmpl6L7el/model.ckpt.\n",
      "Loss for final step: 0.0023459.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x7f134c1c2fd0>, 'hidden_units': [512, 128], 'feature_columns': (_RealValuedColumn(column_name='', dimension=784, default_value=None, dtype=tf.float32, normalizer=None),), 'embedding_lr_multipliers': None, 'optimizer': None, 'dropout': None, 'gradient_clip_norm': None, 'activation_fn': <function relu at 0x7f1301963668>, 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=train_set[0],\n",
    "               y=train_set[1],\n",
    "               batch_size=100,\n",
    "               steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing the Trained Model\n",
    "\n",
    "The last thing we want to do with this model is test the accuracy against some held-out test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting evaluation at 2017-03-10-18:21:46\n",
      "Finished evaluation at 2017-03-10-18:21:46\n",
      "Saving dict for global step 10000: accuracy = 0.9814, auc = 0.998549, global_step = 10000, loss = 0.0636759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.981400\n"
     ]
    }
   ],
   "source": [
    "score = classifier.evaluate(x=test_set[0], y=test_set[1])\n",
    "print('Accuracy: {0:f}'.format(score['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Defining Our Own Model Using `tf.contrib.layers`\n",
    "\n",
    "The next thing we want to illustrate is how someone would implement their own DNN architecture. For this example we will actually just implement the exact same version of the model that we already trained. The purpose will be to get comfortable with defining models using `tf.contrib.layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default config.\n",
      "Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f12efb81190>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "def fully_connected_model(features, labels):\n",
    "    features = layers.flatten(features)\n",
    "    labels = tf.one_hot(tf.cast(labels, tf.int32), 10, 1, 0)\n",
    "\n",
    "    layer1 = layers.fully_connected(features, 512, activation_fn=tf.nn.relu, scope='fc1')\n",
    "    layer2 = layers.fully_connected(layer1, 128, activation_fn=tf.nn.relu, scope='fc2')\n",
    "    logits = layers.fully_connected(layer2, 10, activation_fn=None, scope='out')\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "    train_op = layers.optimize_loss(\n",
    "        loss,\n",
    "        tf.contrib.framework.get_global_step(),\n",
    "        optimizer='SGD',\n",
    "        learning_rate=0.01\n",
    "    )\n",
    "\n",
    "    return tf.argmax(logits, 1), loss, train_op\n",
    "\n",
    "custom_classifier = learn.Estimator(model_fn=fully_connected_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Custom Model\n",
    "\n",
    "Train our custom model with the same steps and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create CheckpointSaverHook.\n",
      "Saving checkpoints for 1 into /tmp/tmpYMw7hX/model.ckpt.\n",
      "loss = 2.35727, step = 1\n",
      "global_step/sec: 279.355\n",
      "loss = 1.4986, step = 101\n",
      "global_step/sec: 287.367\n",
      "loss = 0.900241, step = 201\n",
      "global_step/sec: 282.195\n",
      "loss = 0.625328, step = 301\n",
      "global_step/sec: 281.372\n",
      "loss = 0.618043, step = 401\n",
      "global_step/sec: 281.072\n",
      "loss = 0.49757, step = 501\n",
      "global_step/sec: 286.787\n",
      "loss = 0.455548, step = 601\n",
      "global_step/sec: 288.139\n",
      "loss = 0.4886, step = 701\n",
      "global_step/sec: 288.006\n",
      "loss = 0.426209, step = 801\n",
      "global_step/sec: 287.755\n",
      "loss = 0.315149, step = 901\n",
      "global_step/sec: 283.779\n",
      "loss = 0.411261, step = 1001\n",
      "global_step/sec: 286.017\n",
      "loss = 0.312354, step = 1101\n",
      "global_step/sec: 285.877\n",
      "loss = 0.376406, step = 1201\n",
      "global_step/sec: 285.194\n",
      "loss = 0.333029, step = 1301\n",
      "global_step/sec: 287.648\n",
      "loss = 0.306934, step = 1401\n",
      "global_step/sec: 286.285\n",
      "loss = 0.298749, step = 1501\n",
      "global_step/sec: 289.933\n",
      "loss = 0.283361, step = 1601\n",
      "global_step/sec: 289.549\n",
      "loss = 0.303073, step = 1701\n",
      "global_step/sec: 283.812\n",
      "loss = 0.270855, step = 1801\n",
      "global_step/sec: 289.205\n",
      "loss = 0.352743, step = 1901\n",
      "global_step/sec: 287.462\n",
      "loss = 0.413428, step = 2001\n",
      "global_step/sec: 284.337\n",
      "loss = 0.218779, step = 2101\n",
      "global_step/sec: 287.632\n",
      "loss = 0.337644, step = 2201\n",
      "global_step/sec: 286.38\n",
      "loss = 0.332756, step = 2301\n",
      "global_step/sec: 285.911\n",
      "loss = 0.399042, step = 2401\n",
      "global_step/sec: 285.92\n",
      "loss = 0.422662, step = 2501\n",
      "global_step/sec: 288.784\n",
      "loss = 0.247767, step = 2601\n",
      "global_step/sec: 287.427\n",
      "loss = 0.391569, step = 2701\n",
      "global_step/sec: 287.801\n",
      "loss = 0.154161, step = 2801\n",
      "global_step/sec: 287.123\n",
      "loss = 0.250147, step = 2901\n",
      "global_step/sec: 283.221\n",
      "loss = 0.168333, step = 3001\n",
      "global_step/sec: 287.181\n",
      "loss = 0.249147, step = 3101\n",
      "global_step/sec: 288.724\n",
      "loss = 0.283228, step = 3201\n",
      "global_step/sec: 290.682\n",
      "loss = 0.187627, step = 3301\n",
      "global_step/sec: 289.52\n",
      "loss = 0.148385, step = 3401\n",
      "global_step/sec: 282.445\n",
      "loss = 0.131596, step = 3501\n",
      "global_step/sec: 286.577\n",
      "loss = 0.302728, step = 3601\n",
      "global_step/sec: 287.497\n",
      "loss = 0.185867, step = 3701\n",
      "global_step/sec: 285.262\n",
      "loss = 0.25921, step = 3801\n",
      "global_step/sec: 283.948\n",
      "loss = 0.23791, step = 3901\n",
      "global_step/sec: 285.663\n",
      "loss = 0.255995, step = 4001\n",
      "global_step/sec: 286.29\n",
      "loss = 0.215609, step = 4101\n",
      "global_step/sec: 274.379\n",
      "loss = 0.122382, step = 4201\n",
      "global_step/sec: 285.033\n",
      "loss = 0.238759, step = 4301\n",
      "global_step/sec: 287.923\n",
      "loss = 0.245572, step = 4401\n",
      "global_step/sec: 287.294\n",
      "loss = 0.212269, step = 4501\n",
      "global_step/sec: 287.81\n",
      "loss = 0.136765, step = 4601\n",
      "global_step/sec: 287.375\n",
      "loss = 0.107308, step = 4701\n",
      "global_step/sec: 288.764\n",
      "loss = 0.168439, step = 4801\n",
      "global_step/sec: 288.679\n",
      "loss = 0.201858, step = 4901\n",
      "global_step/sec: 285.867\n",
      "loss = 0.19445, step = 5001\n",
      "global_step/sec: 286.866\n",
      "loss = 0.116113, step = 5101\n",
      "global_step/sec: 287.184\n",
      "loss = 0.227729, step = 5201\n",
      "global_step/sec: 286.789\n",
      "loss = 0.0926163, step = 5301\n",
      "global_step/sec: 285.21\n",
      "loss = 0.196826, step = 5401\n",
      "global_step/sec: 285.904\n",
      "loss = 0.282703, step = 5501\n",
      "global_step/sec: 289.297\n",
      "loss = 0.17687, step = 5601\n",
      "global_step/sec: 287.552\n",
      "loss = 0.127155, step = 5701\n",
      "global_step/sec: 285.679\n",
      "loss = 0.226963, step = 5801\n",
      "global_step/sec: 287.674\n",
      "loss = 0.15574, step = 5901\n",
      "global_step/sec: 288.931\n",
      "loss = 0.283455, step = 6001\n",
      "global_step/sec: 289.077\n",
      "loss = 0.160224, step = 6101\n",
      "global_step/sec: 283.966\n",
      "loss = 0.11669, step = 6201\n",
      "global_step/sec: 287.547\n",
      "loss = 0.152865, step = 6301\n",
      "global_step/sec: 285.812\n",
      "loss = 0.0387692, step = 6401\n",
      "global_step/sec: 285.135\n",
      "loss = 0.173787, step = 6501\n",
      "global_step/sec: 286.527\n",
      "loss = 0.0989311, step = 6601\n",
      "global_step/sec: 286.946\n",
      "loss = 0.108085, step = 6701\n",
      "global_step/sec: 285.425\n",
      "loss = 0.309984, step = 6801\n",
      "global_step/sec: 289.128\n",
      "loss = 0.180993, step = 6901\n",
      "global_step/sec: 280.742\n",
      "loss = 0.105306, step = 7001\n",
      "global_step/sec: 279.789\n",
      "loss = 0.234367, step = 7101\n",
      "global_step/sec: 285.125\n",
      "loss = 0.193589, step = 7201\n",
      "global_step/sec: 286.954\n",
      "loss = 0.103405, step = 7301\n",
      "global_step/sec: 287.672\n",
      "loss = 0.132179, step = 7401\n",
      "global_step/sec: 285.01\n",
      "loss = 0.239851, step = 7501\n",
      "global_step/sec: 288.765\n",
      "loss = 0.216471, step = 7601\n",
      "global_step/sec: 286.378\n",
      "loss = 0.238113, step = 7701\n",
      "global_step/sec: 288.586\n",
      "loss = 0.114735, step = 7801\n",
      "global_step/sec: 287.439\n",
      "loss = 0.0679777, step = 7901\n",
      "global_step/sec: 286.289\n",
      "loss = 0.161877, step = 8001\n",
      "global_step/sec: 286.207\n",
      "loss = 0.0610187, step = 8101\n",
      "global_step/sec: 286.421\n",
      "loss = 0.188088, step = 8201\n",
      "global_step/sec: 285.759\n",
      "loss = 0.159797, step = 8301\n",
      "global_step/sec: 284.615\n",
      "loss = 0.143511, step = 8401\n",
      "global_step/sec: 285.653\n",
      "loss = 0.077006, step = 8501\n",
      "global_step/sec: 287.953\n",
      "loss = 0.155839, step = 8601\n",
      "global_step/sec: 287.555\n",
      "loss = 0.124314, step = 8701\n",
      "global_step/sec: 286.772\n",
      "loss = 0.0708222, step = 8801\n",
      "global_step/sec: 286.777\n",
      "loss = 0.128725, step = 8901\n",
      "global_step/sec: 280.581\n",
      "loss = 0.0921299, step = 9001\n",
      "global_step/sec: 284.166\n",
      "loss = 0.103949, step = 9101\n",
      "global_step/sec: 285.56\n",
      "loss = 0.0901934, step = 9201\n",
      "global_step/sec: 286.836\n",
      "loss = 0.090753, step = 9301\n",
      "global_step/sec: 285.112\n",
      "loss = 0.176367, step = 9401\n",
      "global_step/sec: 287.339\n",
      "loss = 0.128699, step = 9501\n",
      "global_step/sec: 285.246\n",
      "loss = 0.0893943, step = 9601\n",
      "global_step/sec: 283.959\n",
      "loss = 0.158151, step = 9701\n",
      "global_step/sec: 286.996\n",
      "loss = 0.0833036, step = 9801\n",
      "global_step/sec: 288.237\n",
      "loss = 0.086188, step = 9901\n",
      "Saving checkpoints for 10000 into /tmp/tmpYMw7hX/model.ckpt.\n",
      "Loss for final step: 0.0770179.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Estimator(params=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_classifier.fit(x=train_set[0],\n",
    "                      y=train_set[1],\n",
    "                      batch_size=100,\n",
    "                      steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting evaluation at 2017-03-10-18:22:22\n",
      "Finished evaluation at 2017-03-10-18:22:22\n",
      "Saving dict for global step 10000: accuracy = 0.9555, global_step = 10000, loss = 0.14439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.955500\n"
     ]
    }
   ],
   "source": [
    "# Model Testing\n",
    "score = custom_classifier.evaluate(x=test_set[0], y=test_set[1],\n",
    "                                   metrics={'accuracy': MetricSpec(tfmetrics.streaming_accuracy)})\n",
    "print('Accuracy: {0:f}'.format(score['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Summary and Note on Accuracy\n",
    "\n",
    "This is a great starting point for going deeper with TensorFlow. What we have done is implemented a DNN using a pre-built model using `tf.contrib.learn` and our own model using `tf.contrib.layers`. Moving forward, we will expand on the custom model. To look at the raw code of the final model, look at the `model.py` file found in this repository.\n",
    "\n",
    "One thing that I want to address is the fact that the accuracy of the custom model may be lower than the pre-built model. We will cover optimizers in future posts, but long story short is the optimizer used by the DNNClassifier class converges better on this dataset. One might also note that we never used a learning rate when defining the DNNClassifier. It is because we used an adagrad optimizer in the DNNClassifier, but we use a traditional Stochastic Gradient Descent (SGD) optimizer for our custom model. We will go over these a little more in later posts."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
